{"author":{"id":"47f9413dc69eb484d6a9eb4ca38b2e384c249acef65a97e0c95e888e7e568ce9"},"ops":[{"type":3,"timestamp":1536245207,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDQxOTEyMTU5NQ==","github-url":"https://github.com/MichaelMure/git-bug/issues/5#issuecomment-419121595"},"nonce":"plpYZS5l5bUp2xsLu+1GrLztkZw=","message":"@MichaelMure This test case really isn't meaningful. You're just testing how a given payload compresses with zlib when creating loose objects, since when you add a new object it's compressed, a header is added to it, and it's added to the object store.\n\nInstead, you should after every addition do `git add \u0026\u0026 git commit \u0026\u0026 git gc`. Then measure the total size of the now-packed .git/objects directory, not individual objects.\n\nAt that point, these objects will be delta-compressed, so you can see how the size of the repo grows as they're added.\n\nThe size of individual objects is pretty much irrelevant. You can have 10 objects that are all 1GB, but delta-compress down to 1GB + 1MB or whatever, or 10GB if they don't delta-compress at all.","files":null},{"type":3,"timestamp":1536245566,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDQxOTEyMzkzMg==","github-url":"https://github.com/MichaelMure/git-bug/issues/5#issuecomment-419123932"},"nonce":"i+s/9sjmIqjp9cFtLuZpBgR8qJI=","message":"@MichaelMure Also in reply to:\n\n\u003e Who knows how the git compression behave on something that is already binary.\n\nI'm sure there's some obscure edge case where the compression is tweaked for textual content in some way that'll prove me wrong, but in general this doesn't matter at all.\n\nGit's just as good at delta-compressing binary data and non-binary data. What it's not good at compressing (and this goes for any compression), is data that's wildly different from one object to the next.\n\nIt just so happens that *generally* binary data is less delta-compressible, think say two *.mp3s with different songs v.s. a *.txt change to its lyrics.\n\nBut for these sort of pack formats I wouldn't expect them to delta-compress any worse than say JSON. It's going to be other things that matter, e.g. let's say you use a JSON encoder  where the keys of the payload aren't sorted, and thus are different every time, that'll compress worse than if they're sorted, same for doing the same in some binary key-value format.\n\nI do think that for UI purposes it makes sense to pick a widely implemented \u0026 used text format like JSON for introspection purposes and the availability of tooling (e.g. `jq`), if the compression numbers for it aren't much worse that is.","files":null}]}